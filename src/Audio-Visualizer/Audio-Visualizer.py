import pyaudiowpatch as pyaudio
import numpy as np
import sounddevice as sd
import numpy as np
from textual import work
from textual.app import App, ComposeResult
from textual_plotext import PlotextPlot
from textual.worker import Worker

#TODO possibly reinvent the wheel with this: https://medium.com/geekculture/real-time-audio-wave-visualization-in-python-b1c5b96e2d39

# a QThread for audio data reading
class AudioData():

    def __init__(self):
        self.p=pyaudio.PyAudio() #start PyAudio instance
        self.stream:pyaudio.Stream

        # TODO: make rate and channels variable based on the audio device configuration
        self.CHUNK = 2**11       #num of data points read at one time
        self.RATE = 48000        #time resolution of the recording device (Hz)
        self.CHANNELS = 2        #stereo usually has 2 channels
        self.maxValue = 2**15
        self.bars = 50

        default_output = sd.query_devices(kind='output')    #find default audio device
        print(f"Default: {default_output['name']}")

        #for all devices connected, compare names. If default device name has the word "Loopback", set as PyAudio device index
        for i in range(self.p.get_device_count()):
            dev = self.p.get_device_info_by_index(i)
            print ( i, dev.get('name'))
            if default_output['name'] in dev.get('name') and "[Loopback]" in dev.get('name'):
                print (f"This is default device from array {dev.get('name')} at index {i}")
                dev_index = i   #assign dev index to current index

        self.stream = self.p.open(format=pyaudio.paInt16,channels=self.CHANNELS, rate=self.RATE, input=True, input_device_index=dev_index, frames_per_buffer=self.CHUNK)

    # get the left and right data
    def get_lr_data(self):
        data = np.frombuffer(self.stream.read(1024),dtype=np.int16)
        dataL = data[0::2]
        dataR = data[1::2]
        return dataL, dataR

    # close audio stream and terminate pyaudio
    def close(self):
        self.stream.stop_stream()
        self.stream.close()
        self.p.terminate()

class TUI_APP(App[None]):

    def __init__(self):
        super().__init__()
        self.audio_data = AudioData()
        self.peak_plot = PlotextPlot()
        self.spec_plot = PlotextPlot()

        # how long the graph is
        self.num_samples = 50

        # create list for each audio channel peak
        self.left_peak_audio_levels = [0] * self.num_samples
        self.right_peak_audio_levels = [0] * self.num_samples
        
        # how many samples generated by rfttn
        self.num_spec_bytes = 513

        self.left_spec_audio_levels = np.array([0] * self.num_spec_bytes)
        self.right_spec_audio_levels = np.array([0] * self.num_spec_bytes)

        # create horizontal list i.e x-axis
        self.spec_x = [i for i in range(self.num_spec_bytes)]

    def compose(self) -> ComposeResult: # setup the UI
        yield self.peak_plot
        yield self.spec_plot

    def on_mount(self) -> None:
        self.peak_plot.plt.title("Peak Plot")
        self.spec_plot.plt.title("Spec Plot")
        self.update_tui_plots()

    @work(exclusive=False, thread=True)
    async def update_tui_plots(self):
        while (self.is_running):
            lrdata = self.audio_data.get_lr_data()
            dataL, dataR = lrdata
            peakL = np.abs(np.max(dataL)-np.min(dataL))/self.audio_data.maxValue
            peakR = -1 * np.abs(np.max(dataR)-np.min(dataR))/self.audio_data.maxValue

            self.left_peak_audio_levels.pop(0)
            self.right_peak_audio_levels.pop(0)

            self.left_peak_audio_levels.append(peakL)
            self.right_peak_audio_levels.append(peakR)

            # alpha filter smoothing formula: smoothed = (new_data * alpha) + (last_smoothed * beta)
            #                                   where last_smoothed = the smoothed value from the previous new_data collection
            #                                   alpha is a float percent (0.0 through 1.0)
            #                                   beta = 1 - alpha
            # parameters for alpha filter
            alpha = 0.5 # or a 50% weight with the last sample
            beta = 1 - alpha

            # spectrogram is tough and requires FAST FORIER TRANSFORMS which is diffeq stuff so we'll let numpy handle the specifics
            # learned what i needed from this https://www.yhoka.com/en/posts/fft-python/

            dataL_y = np.abs(np.fft.rfftn(dataL).astype(np.float64))
            dataL_x = np.fft.rfftfreq(len(dataL), 1 / self.audio_data.RATE)
            self.left_spec_audio_levels = (dataL_y * alpha) + (self.left_spec_audio_levels * beta) # applying alpha filter

            dataR_y = -1 * np.abs(np.fft.rfftn(dataR).astype(np.float64))
            dataR_x = np.fft.rfftfreq(len(dataR), 1 / self.audio_data.RATE)
            self.right_spec_audio_levels = (dataR_y * alpha) + (self.right_spec_audio_levels * beta) # applying alpha filter

            self.call_from_thread(self.peak_plot.plt.clear_data)
            self.call_from_thread(self.spec_plot.plt.clear_data)
            
            self.call_from_thread(self.peak_plot.plt.ylim, -0.50, 0.50)
            self.call_from_thread(self.spec_plot.plt.ylim, -900000, 900000)
            self.call_from_thread(self.spec_plot.plt.xlim, 0, 10000)

            self.call_from_thread(self.peak_plot.plt.bar, self.left_peak_audio_levels)
            self.call_from_thread(self.peak_plot.plt.bar, self.right_peak_audio_levels)

            self.call_from_thread(self.spec_plot.plt.bar, dataL_x, self.left_spec_audio_levels)
            self.call_from_thread(self.spec_plot.plt.bar, dataR_x, self.right_spec_audio_levels)

    def on_worker_state_changed(self, event: Worker.StateChanged) -> None:
        """Called when the worker state changes."""
        self.log(event)

# main method
if __name__ == "__main__":
    tui_app = TUI_APP()

    tui_app.run()
    
